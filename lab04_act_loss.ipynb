{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shkwo-n/news_words_analysis/blob/main/lab04_act_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 4 â€“ Activation and Loss Functions \n",
        "- We are going to see a few activation and loss functions called from PyTorch and also written from scratch (in plain Python)\n",
        "- We will plot the graphs for these functions"
      ],
      "metadata": {
        "id": "P5ecJOVMqnOd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tzde8UV2gk1B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax Function\n",
        "- The main function to use would be `torch.nn.Softmax()`\n",
        "- It is also available as `torch.softmax()` and `torch.nn.functional.softmax()`"
      ],
      "metadata": {
        "id": "xiAYvbHBrFYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use this sample data\n",
        "x = torch.tensor([-2.0, 0, 1.0, 3.0])\n",
        "\n",
        "# Softmax from PyTorch\n",
        "sm = nn.Softmax(dim=0) # Also torch.softmax(x, dim=0)\n",
        "print(\"Softmax output of x\")\n",
        "output = sm(x)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU5cckosgvyq",
        "outputId": "59ae973b-36b3-4f0c-80ae-8033c85ac32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax output of x\n",
            "tensor([0.0057, 0.0418, 0.1135, 0.8390])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1. Sum up the values in output\n",
        "# CODE\n",
        "\n",
        "# Q2. Why do they sum up to the value in Q1?\n",
        "# TEXT\n"
      ],
      "metadata": {
        "id": "DJJxCIEHr79m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax from scratch"
      ],
      "metadata": {
        "id": "nvjspUPWsEm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax using plain Python (should give same result as before)\n",
        "def my_softmax(nums):\n",
        "    t = np.exp(nums)\n",
        "    t = t / t.sum(axis=0, keepdims=False)\n",
        "    return t\n",
        "\n",
        "soft = my_softmax(x)\n",
        "print(soft)"
      ],
      "metadata": {
        "id": "0WmhOMdEoXII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. If x represents the raw output (logits) of a neural network, with 4 classes (0-3), which class would be returned by the softmax classifier?\n",
        "# TEXT"
      ],
      "metadata": {
        "id": "BgencSI5Z3Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's plot what this softmax function looks like"
      ],
      "metadata": {
        "id": "3QzdVUuBsbDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot tensors values (x) against softmax values (y)\n",
        "plt.style.use('ggplot')\n",
        "plt.plot(x, soft, linewidth=3.0)"
      ],
      "metadata": {
        "id": "wYf1HYJFpout"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. What do you understand from this graph?\n",
        "# TEXT"
      ],
      "metadata": {
        "id": "LSB9RLjWqjNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid Function\n",
        "- The main function to use would be `torch.nn.Sigmoid()`\n",
        "- It is also available as `torch.sigmoid()` and `torch.nn.functional.sigmoid()`"
      ],
      "metadata": {
        "id": "oZwp4wLAs6Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid from PyTorch\n",
        "s = nn.Sigmoid() # Also torch.sigmoid(x)\n",
        "output = s(x)\n",
        "print(\"Sigmoid output of x\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "v6tsa9C4oeX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5. Sum up the values in output\n",
        "# CODE\n",
        "\n",
        "# Q6. Do they sum to one? Why?\n",
        "# TEXT\n"
      ],
      "metadata": {
        "id": "hEvdq83holVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sigmoid from scratch"
      ],
      "metadata": {
        "id": "mrwHyFqQtbzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid in plain Python (should give same result as before)\n",
        "\n",
        "def my_sigmoid(z):\n",
        "    result = 1 / (1 + np.exp(-z))\n",
        "    return result\n",
        "\n",
        "sig = my_sigmoid(x)\n",
        "print(sig)"
      ],
      "metadata": {
        "id": "mZENkMqnkjyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's plot what this sigmoid function looks like"
      ],
      "metadata": {
        "id": "wJ3qUgOktlhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Q7. Plot x on the x axis and the their sigmoid values on the y axis\n",
        "# CODE\n"
      ],
      "metadata": {
        "id": "F36UOBG_k20g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q8. Does the curve look like what you've seen in the lecture?\n",
        "# TEXT"
      ],
      "metadata": {
        "id": "AzRKBFXPuKfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get more data and see if we can get a more \"S-shaped\" curve"
      ],
      "metadata": {
        "id": "s-4Vzgf-pKG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a range of numbers from -10 to 10\n",
        "newx = torch.arange(-10., 10., 0.2)\n",
        "\n",
        "# Q9. Get their sigmoid values using my_sigmoid()\n",
        "# CODE\n",
        "\n",
        "# Q10. Plot newx against their sigmoid values\n",
        "# CODE\n"
      ],
      "metadata": {
        "id": "HLD49P-hpPNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q11. Does the graph look more like what you expect?\n",
        "# TEXT"
      ],
      "metadata": {
        "id": "NvRyouyOurso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperbolic Tan Function\n",
        "- The main function to use would be `torch.nn.Tanh()`\n",
        "- It is also available as `torch.tanh()` and `torch.nn.functional.tanh()`\n"
      ],
      "metadata": {
        "id": "Yg8yGA3TWScd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tanh from PyTorch. Also F.tanh()\n",
        "print(\"Hyperbolic tan output of x\")\n",
        "t = nn.Tanh() # Also torch.tanh(x)\n",
        "output = t(x)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "qgYfWskblYq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the tanh graph"
      ],
      "metadata": {
        "id": "1ZA1gMone2F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('ggplot')\n",
        "plt.plot(x, output, linewidth=3.0)"
      ],
      "metadata": {
        "id": "wJa4R0WL5dpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we do not see a clear S-shape. Let's try to use more data and derive tanh from sigmoid.\n",
        "\n",
        "### Tanh from Sigmoid"
      ],
      "metadata": {
        "id": "RMCP7qPaWElU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is your new data\n",
        "newt = torch.arange(-10., 10., 0.2)\n",
        "\n",
        "# Q12. Calculate tanh using my_sigmoid() above. Remember that tanh = 2*(2*sigmoid(2x) - 1\n",
        "# You may want to use np.dot()\n",
        "# CODE\n",
        "\n",
        "# Q13. Plot newt against their tanh values derived using sigmoid\n"
      ],
      "metadata": {
        "id": "D7-M2tc3lnV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q14. What's the difference between this graph and the graph for sigmoid in Q10?\n",
        "# TEXT\n"
      ],
      "metadata": {
        "id": "GoHgoOJYfPse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReLU\n",
        "- The main function to use would be `torch.nn.ReLU()`\n",
        "- It is also available as `torch.relu()` and `torch.nn.functional.relu()`"
      ],
      "metadata": {
        "id": "WTXGu-w0ZGgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU from PyTorch\n",
        "x = torch.tensor([-2.0, 0, 1.0, 3.0])\n",
        "\n",
        "# First a ReLU instance will need to be created\n",
        "relu = nn.ReLU() # Also torch.relu(x)\n",
        "# Call the instance on the data\n",
        "output = relu(x)\n",
        "print(\"ReLU output of x\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "IdvoJuvbhltu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReLU from scratch"
      ],
      "metadata": {
        "id": "xtFEbDiyZZAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU using plain Python\n",
        "x = torch.tensor([-2.0, 0, 1.0, 3.0])\n",
        "\n",
        "relu = [max(0, i) for i in x]\n",
        "print(relu)\n",
        "\n",
        "# Plot x against their ReLU values\n",
        "plt.style.use('ggplot')\n",
        "plt.plot(x, relu, linewidth=3.0)\n",
        "\n"
      ],
      "metadata": {
        "id": "YCXaBtQomqrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leaky ReLU\n",
        "- The main function to use would be `torch.nn.LeakyReLU()`\n",
        "- It is also available as `torch.nn.functional.leaky_relu()`\n",
        "- PyTorch documentation: https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html"
      ],
      "metadata": {
        "id": "1kF2qD_zZmYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# leaky relu\n",
        "x1 = torch.tensor([-5.0, 0, 1.0, 3.0])\n",
        "\n",
        "# Q14. Calculate and print the leaky ReLU function for x1 using the main function for it in PyTorch\n",
        "# Follow the same conventions as ReLU.\n",
        "# CODE\n",
        "\n"
      ],
      "metadata": {
        "id": "__pj4sSr51pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q15. Given that the function for leaky ReLU is f(x) = a * x if x < 0; x otherwise, what is the value of 'a' from output of your function in Q14?\n",
        "# MATHS / CODE\n",
        "\n",
        "# Q16. What does 'a' represent in the graph?\n",
        "# TEXT"
      ],
      "metadata": {
        "id": "JmEtOOPKar4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the Leaky ReLU Function"
      ],
      "metadata": {
        "id": "Lv1kzbOqaAFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q17. Plot x1 against their Leaky ReLU values\n"
      ],
      "metadata": {
        "id": "1wZxoYJ5aDJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The graph may not look too different to ReLU. This is because of the range of the dataset we're using and the slope. Let's try to highlight the difference by adjusting these.\n",
        "\n",
        "## Plot with more data"
      ],
      "metadata": {
        "id": "c2JeXjy5bI6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's use this data with a larger range\n",
        "newl = torch.arange(-10., 10., 0.2)\n",
        "\n",
        "# Q18. Call the main LeakyReLU function on newl with the negative_slope argument set to 0.2\n",
        "\n",
        "# Q19. Plot newl against their leaky relu values from Q17.\n"
      ],
      "metadata": {
        "id": "kHslOC7wbONH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q20. What would you need to set negative_slope to in order for this graph to be a straight line?\n",
        "# TEXT"
      ],
      "metadata": {
        "id": "ppnKttGEkMew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Functions\n",
        "We have seen Mean Squared Error (MSE) in action last week. It is one of the main loss functions for regression problems. Let's explore a few loss functions for regression and classification."
      ],
      "metadata": {
        "id": "tUzxrDYeoMZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Squared Error (MSE) Loss / L2 Loss\n",
        "- Computes the average of the squared differences between actual values and predicted values.\n",
        "- Loss$(y, \\hat{y}) = (y-\\hat{y})^{2}$\n",
        "- Always outputs a positive result, regardless of the sign of actual and predicted values. \n",
        "- The squaring implies that larger mistakes produce even larger errors than smaller ones. If the difference is off by 100, the error is 10,000. If itâ€™s off by 0.1, the error is 0.01. This punishes the model for making big mistakes and encourages small mistakes. \n",
        "- To enhance the accuracy of the model, you should try to reduce the L2 Lossâ€”a perfect value is 0.0\n",
        "- Used for: Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "l-1Y6x0Zp6tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE Loss / L2 Loss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input = torch.tensor([-2.0, 0, 1.0, 3.0], requires_grad=True)\n",
        "target = torch.tensor([-2.5, 0.3, 1.7, 5.0])\n",
        "\n",
        "mse_loss = nn.MSELoss()\n",
        "output = mse_loss(input, target)\n",
        "output.backward()\n",
        "\n",
        "print('input: ', input)\n",
        "print('target: ', target)\n",
        "print('output: ', output)\n"
      ],
      "metadata": {
        "id": "0MqDUy0rpzph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mean Absolute Error (MAE) Loss / L1 Loss\n",
        "- Computes the average of the sum of absolute differences between actual values and predicted values.\n",
        "- Loss$(y, \\hat{y}) = |y-\\hat{y}|$\n",
        "- Checks the size of errors in a set of predicted values, without caring about their positive or negative direction.\n",
        "- Considered to be more robust to outliers.\n",
        "- Used for: Regression\n",
        "\n"
      ],
      "metadata": {
        "id": "78VkkQGcrjKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "target = torch.tensor([-2.5, 0.3, 1.7, 5.0])\n",
        "\n",
        "mae_loss = nn.L1Loss()\n",
        "output = mae_loss(input, target)\n",
        "output.backward()\n",
        "\n",
        "print('input: ', input)\n",
        "print('target: ', target)\n",
        "print('output: ', output)"
      ],
      "metadata": {
        "id": "3B7liiSLo2mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary Cross Entropy Loss / Log Loss\n",
        "- We want to maximise the probability of the predicted class to be the same as the correct class output given our input\n",
        "- However, to maximise this class probability, a product of the probability of the output class given the input for all the samples will need to be calculated (Maximum Likelihood Estimation)\n",
        "- Transforming these values to logs will turn this into a sum and also stabilise the computation (maximise log likelihood)\n",
        "- On top of that, we multiply by -1 so that it now becomes a minimisation (of the negative log likelihood) problem, which will be quite convenient using gradient descent (next class).\n",
        "- Think of it like changing from maximising the accuracy rate to minimising the error rate.\n",
        "- Remember, ultimately, we are doing all this to optimise the weights and biases, which characterise the model\n",
        "- Used for: Binary classification"
      ],
      "metadata": {
        "id": "2zZ6DxllukQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "## BINARY CLASSIFICATION\n",
        "\n",
        "# Actual labels\n",
        "# NOTE: you do not need to one-hot encode the labels. The loss function expect an integer value with the corresponding class. \n",
        "labels = torch.tensor([1, 0, 1, 1, 1, 0], dtype=torch.float)\n",
        "# Logits: raw output of the NN\n",
        "logits = torch.tensor([2.5, -1.1, 1.2, 2.2, 0.1, -0.5], dtype=torch.float)\n",
        "\n",
        "# Change logits to probabilities by running them over sigmoid\n",
        "probs = torch.sigmoid(logits)\n",
        "\n",
        "# Calculate BCE loss\n",
        "bce = nn.BCELoss() # also torch.nn.functional.binary_cross_entropy()\n",
        "output = bce(probs, labels)\n",
        "print(output)\n",
        "# tensor(0.3088)"
      ],
      "metadata": {
        "id": "-pONPmk_60OE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q21. Redo the above using either torch.nn.BCEWithLogitsLoss() or torch.nn.functional.binary_cross_entropy_with_logits()\n",
        "# CODE"
      ],
      "metadata": {
        "id": "R2yeoFcCQhcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BCE from scratch\n",
        "Source: https://github.com/rasbt/machine-learning-notes/blob/main/losses/pytorch-loss-functions/binary-cross-entropy-in-pytorch.ipynb"
      ],
      "metadata": {
        "id": "gC0ByyKmDq4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BCE from scratch\n",
        "def binary_logistic_loss(probas, y_targets):\n",
        "    first = -y_targets.matmul(torch.log(probas)) # -y * log(y_hat)\n",
        "    second = -(1 - y_targets).matmul(torch.log(1 - probas)) # -(1 - y) * log(1 - y_hat)\n",
        "    return (first + second) / y_targets.shape[0] # add and divide by number of samples\n",
        "\n",
        "binary_logistic_loss(probs, labels)"
      ],
      "metadata": {
        "id": "-Ld-ZXyWDJMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross Entropy Loss / Negative Log Likelihood (NLL) Loss\n",
        "- Applied on models with the softmax function as an output activation layer.\n",
        "- Loss$(y_{i}, \\hat{y_{i}}) = - \\sum_{i}^{C} y_{i} \\ \\textrm{log}(\\hat{y_{i}}))$\n",
        "- Called negative (log likelihood) because the softmax probabilities (or likelihoods) vary between 0 and 1, and the logarithms of values in this range are negative.\n",
        "- The model is punished for making the correct prediction with smaller probabilities and encouraged for making the prediction with higher probabilities.\n",
        "- Used for: Multi-class classification"
      ],
      "metadata": {
        "id": "TjLVc3OCsb-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## MULTICLASS CLASSIFICATION\n",
        "# Actual labels. Integer labels of the correct class is provided, e.g. class 1, class 0 and class 2\n",
        "labels = torch.tensor([1, 0, 2], dtype=torch.long)\n",
        "\n",
        "# Predicted logits (not transformed to probability distributions yet)\n",
        "logits = torch.tensor([[2.5, -0.5, 0.1],\n",
        "                        [-1.1, 2.5, 0.0],\n",
        "                        [1.2, 2.2, 3.1]], dtype=torch.float)\n",
        "\n",
        "ce = nn.CrossEntropyLoss() # Also torch.nn.functional.cross_entropy()\n",
        "output = ce(logits, labels)\n",
        "print(output)\n",
        "# tensor(2.4258)\n"
      ],
      "metadata": {
        "id": "bI2iDlbU8KpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q22. Redo the above using torch.nn.functional.nll_loss()\n",
        "# CODE"
      ],
      "metadata": {
        "id": "AZ5GUO9wekjD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}